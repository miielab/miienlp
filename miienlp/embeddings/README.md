# WordEmbeddings
## Description
[Word Embeddings](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) (e.g., [word2vec](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf), [Glove](https://aclanthology.org/D14-1162.pdf)) and Contextual Word Vectors (e.g., [ELMo](https://aclanthology.org/N18-1202.pdf), [BERT](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)) allows one to capture the context of a word within a document, relation to other words, and semanatic and syntactic similarity. 
This pipeline constructs word vector models (word2vec) which convert words into their vector representations, allowing one to compare words using metrics such as cosine similarity. A future implementation will contain code to setup a BERT model.

**IMPORTANT** 

For more details, see:
- [example](https://github.com/miielab/miienlp/blob/main/examples/wordEmbeddings_example.md) 
- [setup instructions for UChicago MiiE Lab RA ONLY](https://github.com/miielab/miienlp/blob/main/documentation/miie_ra_documentation/wordEmbeddings.md)
- [setup instructions for running Word Embeddings locally](https://github.com/miielab/miienlp/blob/main/documentation/user_documentation/wordEmbeddings.md)




